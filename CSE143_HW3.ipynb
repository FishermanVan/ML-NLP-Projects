{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FishermanVan/ML-NLP-Projects/blob/main/CSE143_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkBwO4BX9aT"
      },
      "source": [
        "**Chapter 16 – Natural Language Processing with RNNs and Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcctjNuxX9aZ"
      },
      "source": [
        "_This notebook contains all the sample code in chapter 16._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY2FNgBFX9aZ"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jflanigan/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEhgAdWZX9ab"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjZGu8RJX9ac"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DAW2LPIX9ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c3f691-4fe4-47c0-d364-4b17244f47ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-a71e2dbfd47b>:22: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvJEkVbTX9af"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wiyQAwOX9ag"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lJdHsY8X9ag"
      },
      "source": [
        "You can load the IMDB dataset easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INhPeQ2CX9ah"
      },
      "outputs": [],
      "source": [
        "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9OZU4aaX9ah",
        "outputId": "aff08bed-e580-439c-f3e5-eb6104ab5ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X_train[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAGpdgy3X9ai",
        "outputId": "afa383a1-dc20-402a-d1d8-e4e0b4a2945e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<sos> this film was just brilliant casting location scenery story'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6OuhNSpX9aj"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2KAu1vhX9aj",
        "outputId": "7158125f-a370-4387-89f3-c33ae3da254e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['test', 'train', 'unsupervised'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "datasets.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd8MD2LSX9ak"
      },
      "outputs": [],
      "source": [
        "train_size = info.splits[\"train\"].num_examples\n",
        "test_size = info.splits[\"test\"].num_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHP1M59ZX9ak",
        "outputId": "500a627c-4aa2-4e31-e272-8fd8812d144c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_size, test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4QhgvypX9ak",
        "outputId": "d328e015-2433-4491-9879-292657a7001e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2A_CqvxX9al"
      },
      "outputs": [],
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    #print(X_batch[1])\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
        "\n",
        "# def filter_short_reviews_preprocess(X_batch, y_batch):\n",
        "#     print(\"The length is\", len(X_batch[4]))\n",
        "#     X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "#     X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "#     X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "#     X_batch = tf.strings.split(X_batch)\n",
        "#     return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3b7Fx2kX9al",
        "outputId": "e0a16104-712c-4bab-aa34-51b7095f3914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
              " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
              "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
              "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
              "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
              "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
              "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
              "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
              "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
              "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
              "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
              "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
              "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
              "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
              "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
              "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
              "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
              "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
              "         b'Cons']], dtype=object)>,\n",
              " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "preprocess(X_batch, y_batch)\n",
        "#filter_short_reviews_preprocess(X_batch, y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bwKKsnXX9al"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaPe3CLJX9am",
        "outputId": "61ed00d5-5a58-43cf-9db0-334d82b53ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "vocabulary.most_common()[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ3fYeLtX9am",
        "outputId": "5b7af649-aa6b-4ba8-8daa-76123fab2f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53893"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z54wSIzX9am"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oubp1NyPX9am",
        "outputId": "178ff506-643f-4090-d5ef-0a78b31dfef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "12\n",
            "11\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This movie was faaaaaantastic\".split():\n",
        "    print(word_to_id.get(word) or vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO3mP3g_X9an"
      },
      "outputs": [],
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSchxURXX9an",
        "outputId": "e01af789-6b71-4c88-96a6-e413ddf58d49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69KplGZJX9an"
      },
      "outputs": [],
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH3xCsBJX9an",
        "outputId": "7d5ad6ea-c556-41d8-deeb-5c9d982dc77b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  22   11   28 ...    0    0    0]\n",
            " [   6   21   70 ...    0    0    0]\n",
            " [4099 6881    1 ...    0    0    0]\n",
            " ...\n",
            " [  22   12  118 ...  331 1047    0]\n",
            " [1757 4101  451 ...    0    0    0]\n",
            " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
            "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMCFp-6VX9ao",
        "outputId": "8d3bdbfd-595b-468f-c5fa-bf6f817ba63c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "781/781 [==============================] - 22s 19ms/step - loss: 0.5565 - accuracy: 0.7006\n",
            "Epoch 2/5\n",
            "781/781 [==============================] - 15s 19ms/step - loss: 0.3517 - accuracy: 0.8521\n",
            "Epoch 3/5\n",
            "781/781 [==============================] - 14s 18ms/step - loss: 0.1879 - accuracy: 0.9332\n",
            "Epoch 4/5\n",
            "781/781 [==============================] - 14s 18ms/step - loss: 0.1365 - accuracy: 0.9514\n",
            "Epoch 5/5\n",
            "781/781 [==============================] - 14s 18ms/step - loss: 0.1141 - accuracy: 0.9579\n"
          ]
        }
      ],
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTP4hJSIX9ao"
      },
      "source": [
        "Or using manual masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtLogukyX9ao",
        "outputId": "7a07725d-0940-4927-fd33-ecddd33190ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "781/781 [==============================] - 21s 19ms/step - loss: 0.5580 - accuracy: 0.7049\n",
            "Epoch 2/5\n",
            "781/781 [==============================] - 15s 19ms/step - loss: 0.3841 - accuracy: 0.8321\n",
            "Epoch 3/5\n",
            "781/781 [==============================] - 14s 18ms/step - loss: 0.2388 - accuracy: 0.9081\n",
            "Epoch 4/5\n",
            "781/781 [==============================] - 14s 18ms/step - loss: 0.1447 - accuracy: 0.9482\n",
            "Epoch 5/5\n",
            "781/781 [==============================] - 14s 18ms/step - loss: 0.1225 - accuracy: 0.9546\n"
          ]
        }
      ],
      "source": [
        "K = keras.backend\n",
        "embed_size = 128\n",
        "inputs = keras.layers.Input(shape=[None])\n",
        "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
        "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
        "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
        "z = keras.layers.GRU(128)(z, mask=mask)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVlyrOUYX9ao"
      },
      "source": [
        "## Reusing Pretrained Embeddings(Dont run this part if you want the Assignment Code to work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upaSEvMTX9ap"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp8od9TkX9ap"
      },
      "outputs": [],
      "source": [
        "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8-nJGUxX9ap"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "model = keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
        "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDT8quzsX9ap",
        "outputId": "de35c1e2-2448-4ff8-874d-b6cc3ca7801e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n"
          ]
        }
      ],
      "source": [
        "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirpath, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8HpA4s3X9ap"
      },
      "outputs": [],
      "source": [
        "# import tensorflow_datasets as tfds\n",
        "\n",
        "# datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "# train_size = info.splits[\"train\"].num_examples\n",
        "# batch_size = 32\n",
        "# train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
        "# history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 3**"
      ],
      "metadata": {
        "id": "MI3mJW4Iasaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1.1 Programming: Text Classification with RNNs(20%)"
      ],
      "metadata": {
        "id": "6s1RE7IKbMJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "74adc397-9893-4465-92a9-539f37699530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6VaMr-XbWph"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "781/781 [==============================] - 94s 117ms/step - loss: 0.6958 - accuracy: 0.5020\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 88s 113ms/step - loss: 0.6879 - accuracy: 0.5416\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 87s 112ms/step - loss: 0.6463 - accuracy: 0.6196\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 86s 111ms/step - loss: 0.5563 - accuracy: 0.7109\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 86s 110ms/step - loss: 0.4846 - accuracy: 0.7631\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 86s 110ms/step - loss: 0.3983 - accuracy: 0.8196\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 87s 111ms/step - loss: 0.3367 - accuracy: 0.8549\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 85s 109ms/step - loss: 0.2923 - accuracy: 0.8737\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 86s 111ms/step - loss: 0.2629 - accuracy: 0.8894\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 86s 111ms/step - loss: 0.2330 - accuracy: 0.9039\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 87s 111ms/step - loss: 0.2154 - accuracy: 0.9136\n",
            "Epoch 12/20\n",
            "781/781 [==============================] - 85s 109ms/step - loss: 0.1984 - accuracy: 0.9201\n",
            "Epoch 13/20\n",
            "781/781 [==============================] - 86s 110ms/step - loss: 0.1826 - accuracy: 0.9269\n",
            "Epoch 14/20\n",
            "781/781 [==============================] - 87s 111ms/step - loss: 0.1752 - accuracy: 0.9310\n",
            "Epoch 15/20\n",
            "781/781 [==============================] - 86s 111ms/step - loss: 0.1720 - accuracy: 0.9318\n",
            "Epoch 16/20\n",
            "781/781 [==============================] - 86s 110ms/step - loss: 0.1547 - accuracy: 0.9408\n",
            "Epoch 17/20\n",
            "781/781 [==============================] - 87s 111ms/step - loss: 0.1509 - accuracy: 0.9421\n",
            "Epoch 18/20\n",
            "781/781 [==============================] - 86s 110ms/step - loss: 0.1447 - accuracy: 0.9434\n",
            "Epoch 19/20\n",
            "781/781 [==============================] - 86s 110ms/step - loss: 0.1366 - accuracy: 0.9480\n",
            "Epoch 20/20\n",
            "179/781 [=====>........................] - ETA: 1:05 - loss: 0.1369 - accuracy: 0.9485"
          ]
        }
      ],
      "source": [
        "### Training Simple RNN model \n",
        "embed_size = 16\n",
        "epochs = 20\n",
        "activation = 'tanh'\n",
        "dropout = 0.5\n",
        "hidden_dimension_size = 64\n",
        "learning_rate=1e-3\n",
        "\n",
        "cell = keras.layers.SimpleRNNCell(hidden_dimension_size, activation=activation, dropout=dropout)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                            mask_zero=True, # not shown in the book\n",
        "                            input_shape=[None]),\n",
        "    keras.layers.RNN(cell),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, steps_per_epoch=train_size // 32, batch_size = 32, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing trained simple RNN model on training set and test set(outputing accuracy of model on training & test set):"
      ],
      "metadata": {
        "id": "yPjcTZVZs2Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start evalutating the accuracy on the training set:\\n\")\n",
        "model.evaluate(train_set, steps=train_size // 32)\n",
        "\n",
        "test_set = datasets[\"test\"].repeat().batch(32).map(preprocess)\n",
        "test_set = test_set.map(encode_words).prefetch(1)\n",
        "\n",
        "print(\"\\nStart evalutating the accuracy on the test set:\\n\")\n",
        "model.evaluate(test_set, steps=test_size // 32)"
      ],
      "metadata": {
        "id": "CIGgLJWwtj1w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "9215d8f4-8b40-40ef-886b-b119512b7862"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start evalutating the accuracy on the training set:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d0b12b6e7f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start evalutating the accuracy on the training set:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1.2 Programming: Text Classification with LSTMs(20%)"
      ],
      "metadata": {
        "id": "CfXv0kNP2Z8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training LSTM model \n",
        "embed_size = 16\n",
        "epochs = 20\n",
        "activation = 'tanh'\n",
        "dropout = 0.5\n",
        "hidden_dimension_size = 64\n",
        "learning_rate=1e-3\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                            mask_zero=True, # not shown in the book\n",
        "                            input_shape=[None]),\n",
        "    keras.layers.LSTM(hidden_dimension_size, activation=activation, dropout=dropout),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, steps_per_epoch=train_size // 32, batch_size = 32, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "T2E4hTsJ3pX8",
        "outputId": "e13d8aea-dce9-41f7-834c-d7f07882e4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_6_input'), name='embedding_6_input', description=\"created by layer 'embedding_6_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_6_input'), name='embedding_6_input', description=\"created by layer 'embedding_6_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-aafc03f0d791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_6\" (type Sequential).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 16)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing trained LSTM model on training set and test set(outputing accuracy of model on training & test set):"
      ],
      "metadata": {
        "id": "zVa4eLtT6pn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start evalutating the accuracy on the training set:\\n\")\n",
        "model.evaluate(train_set, steps=train_size // 32)\n",
        "\n",
        "print(\"\\nStart evalutating the accuracy on the test set:\\n\")\n",
        "model.evaluate(test_set, steps=test_size // 32)"
      ],
      "metadata": {
        "id": "aZ4Gmx4t6vt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "CSE143 HW3",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}